# -*- coding: utf-8 -*-
"""NLP_PROJECT_bbc_news.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a24xD1dAuDIbruiuNwyHgFcbjbJxEMSx
"""

from google.colab import drive
drive.mount('/content/drive')

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import spacy
import re
import pandas as pd
import matplotlib.pyplot as plt

"""#load data"""

bbc_data =pd.read_csv("/content/drive/MyDrive/bbc_news.csv")

bbc_data.head()

bbc_data.info()

titels =pd.DataFrame(bbc_data['title'])

titels.head()

"""#Clean Data"""

#lower case

titels['loewrcase'] =titels['title'].str.lower()

nltk.download("stopwords")
en_stopwords=stopwords.words('english')

# stop word removal
titels['bbc_no_stopwords']=titels['loewrcase'].apply(lambda x:' '.join([word for word in x.split() if word not in (en_stopwords)]))

titels['bbc_no_stopwords']

#punctation remaval

titels['no_stopwords_no_punct']=titels.apply(lambda x: re.sub(r'[*]' ,"star" ,x['bbc_no_stopwords']),axis=1)

titels['no_stopwords_no_punct']

titels['no_stopwords_no_punct']=titels.apply(lambda x: re.sub(r'[*]' ,"star" ,x['bbc_no_stopwords']),axis=1)

nltk.download('punkt')

#tokens
titels['token_raw'] = titels.apply(lambda x: word_tokenize(x['title']), axis=1)

titels['token_raw']

titels['token_clean'] = titels.apply(lambda x: word_tokenize(x['no_stopwords_no_punct']), axis=1)

titels['token_clean']

#leamtizing

leammtizer = WordNetLemmatizer()

nltk.download('wordnet')

titels['lammtized']=titels['tokenized'].apply(lambda tokens: [leammtizer.lemmatize(token) for token in tokens])

titels['lammtized']

tokens_clean_leammtiz = sum(titels['lammtized'], [])

tokens_clean_leammtiz

tokens_clea_raw = sum(titels['token_raw'], [])

tokens_clea_raw

#pos tagging

nlp =spacy.load("en_core_web_sm")

spacy_doc =nlp(' '.join(tokens_clea_raw))

spacy_doc

pos_df = pd.DataFrame(columns=['token','pos_tag'])

for token in spacy_doc:
  pos_df =pos_df.append({'token':token.text ,'pos_tag':token.pos_}, ignore_index=True)

for word in spacy_doc.ents:
  print(word.text , word.label_)

import spacy
from spacy import displacy
displacy.render(spacy_doc,style="ent", jupyter=True)

pos_df_counts=pos_df.groupby(['token','pos_tag']).size().reset_index(name ='counts').sort_values(by='counts' ,ascending=False)

pos_df_counts.head(10)

nouns=pos_df_counts[pos_df_counts.pos_tag=='NOUN'][:20]
nouns

verb=pos_df_counts[pos_df_counts.pos_tag=='VARB'][:10]
verb

ccong=pos_df_counts[pos_df_counts.pos_tag=='CCONJ'][:20]
ccong

part=pos_df_counts[pos_df_counts.pos_tag=='PART'][:20]
part

punct=pos_df_counts[pos_df_counts.pos_tag=='PUNCT'][:20]
punct

"""# NER"""

ner_df = pd.DataFrame(columns=['token','ner_tag'])

for token in spacy_doc.ents:
  if pd.isna(token.label_) is False:
    ner_df =ner_df.append({'token':token.text , 'ner_tag':token.label_},ignore_index=True)

ner_df_counts=ner_df.groupby(['token','ner_tag']).size().reset_index(name ='counts').sort_values(by='counts' ,ascending=False)

ner_df_counts.head()



people = ner_df_counts[ner_df_counts.ner_tag=="PERSON"][:10]

people.head()

